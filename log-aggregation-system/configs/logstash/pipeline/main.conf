# Main Logstash Pipeline Configuration

input {
  # Beats input (Filebeat, Metricbeat, etc.)
  beats {
    port => 5044
    host => "0.0.0.0"
  }
  
  # TCP input for syslog
  tcp {
    port => 5000
    type => "syslog"
  }
  
  # HTTP input for direct API logging
  http {
    port => 8080
    type => "http"
  }
  
  # Dead letter queue for failed events
  dead_letter_queue {
    path => "/usr/share/logstash/data/dead_letter_queue"
  }
}

filter {
  # Add common fields
  mutate {
    add_field => {
      "[@metadata][processed_at]" => "%{+YYYY-MM-dd HH:mm:ss}"
      "[@metadata][logstash_host]" => "%{host}"
    }
  }
  
  # Parse container logs
  if [container] {
    # Extract container information
    grok {
      match => {
        "[log][file][path]" => "/var/lib/docker/containers/%{DATA:container_id}/%{GREEDYDATA}"
      }
    }
    
    # Parse JSON logs from containers
    if [message] =~ /^\s*{.*}\s*$/ {
      json {
        source => "message"
        target => "parsed"
      }
    }
    
    # Add container metadata
    mutate {
      add_field => {
        "log_type" => "container"
        "source_type" => "docker"
      }
    }
  }
  
  # Parse application logs
  if [service] {
    case [service] {
      "webapp" => {
        grok {
          match => {
            "message" => "%{TIMESTAMP_ISO8601:log_timestamp} \[%{LOGLEVEL:log_level}\] %{DATA:logger}: %{GREEDYDATA:log_message}"
          }
        }
      }
      "api" => {
        grok {
          match => {
            "message" => "%{TIMESTAMP_ISO8601:log_timestamp} %{WORD:method} %{URIPATH:path} %{NUMBER:status_code} %{NUMBER:response_time}ms"
          }
        }
        
        mutate {
          convert => {
            "status_code" => "integer"
            "response_time" => "float"
          }
        }
      }
      "database" => {
        grok {
          match => {
            "message" => "%{TIMESTAMP_ISO8601:log_timestamp} \[%{NUMBER:connection_id}\] %{WORD:query_type} %{GREEDYDATA:query}"
          }
        }
      }
    }
    
    # Parse timestamp if extracted
    if [log_timestamp] {
      date {
        match => [ "log_timestamp", "ISO8601" ]
        target => "@timestamp"
      }
    }
  }
  
  # Parse system logs (syslog format)
  if [type] == "syslog" {
    grok {
      match => {
        "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:server} %{DATA:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:syslog_message}"
      }
    }
    
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
      target => "@timestamp"
    }
  }
  
  # Parse nginx access logs
  if [log_type] == "nginx_access" {
    grok {
      match => {
        "message" => "%{NGINXACCESS}"
      }
    }
    
    mutate {
      convert => {
        "response" => "integer"
        "bytes" => "integer"
        "responsetime" => "float"
      }
    }
    
    # GeoIP lookup for client IP
    geoip {
      source => "clientip"
      target => "geoip"
    }
    
    # User agent parsing
    useragent {
      source => "agent"
      target => "user_agent"
    }
  }
  
  # Parse nginx error logs
  if [log_type] == "nginx_error" {
    grok {
      match => {
        "message" => "%{NGINXERROR}"
      }
    }
  }
  
  # Security log analysis
  if [log_type] == "security" {
    # Parse authentication events
    if "authentication" in [message] {
      grok {
        match => {
          "message" => "authentication %{WORD:auth_result} for user %{USER:username} from %{IP:client_ip}"
        }
      }
      
      mutate {
        add_field => {
          "event_type" => "authentication"
          "security_category" => "access_control"
        }
      }
    }
    
    # Parse privilege escalation
    if "sudo" in [message] {
      grok {
        match => {
          "message" => "%{USER:username} : TTY=%{DATA:tty} ; PWD=%{PATH:pwd} ; USER=%{USER:target_user} ; COMMAND=%{GREEDYDATA:command}"
        }
      }
      
      mutate {
        add_field => {
          "event_type" => "privilege_escalation"
          "security_category" => "system_access"
        }
      }
    }
  }
  
  # Error detection and classification
  if [log_level] {
    if [log_level] in ["ERROR", "FATAL", "CRITICAL"] {
      mutate {
        add_field => {
          "is_error" => true
          "severity" => "high"
        }
      }
    } else if [log_level] in ["WARN", "WARNING"] {
      mutate {
        add_field => {
          "is_warning" => true
          "severity" => "medium"
        }
      }
    }
  }
  
  # Performance metrics extraction
  if [response_time] {
    if [response_time] > 5000 {
      mutate {
        add_field => {
          "is_slow_request" => true
          "performance_issue" => "high_latency"
        }
      }
    }
  }
  
  # Anonymize PII data
  if [client_ip] and [client_ip] !~ /^(10\.|192\.168\.|172\.(1[6-9]|2[0-9]|3[01])\.)/ {
    mutate {
      # Hash external IP addresses for privacy
      add_field => {
        "client_ip_hash" => "%{client_ip}"
      }
    }
    
    fingerprint {
      source => "client_ip"
      target => "client_ip_hash"
      method => "SHA256"
    }
    
    mutate {
      remove_field => ["client_ip"]
    }
  }
  
  # Clean up temporary fields
  mutate {
    remove_field => [
      "log_timestamp",
      "syslog_timestamp",
      "[@metadata][beat]",
      "[@metadata][type]",
      "[@metadata][version]",
      "agent",
      "ecs",
      "input",
      "host"
    ]
  }
}

output {
  # Main Elasticsearch output
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
    template => "/usr/share/logstash/templates/logs-template.json"
    template_name => "logs"
    template_overwrite => true
    
    # Index based on log type and date
    if [service] {
      index => "logs-%{service}-%{+YYYY.MM.dd}"
    } else if [log_type] {
      index => "logs-%{log_type}-%{+YYYY.MM.dd}"
    }
  }
  
  # Security logs to separate index
  if [security_category] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "security-logs-%{+YYYY.MM.dd}"
    }
  }
  
  # Error logs to separate index for fast searching
  if [is_error] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "error-logs-%{+YYYY.MM.dd}"
    }
  }
  
  # Debug output (can be removed in production)
  if [@metadata][debug] {
    stdout {
      codec => rubydebug
    }
  }
}
